{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepakgarg08/llm-diary/blob/main/llm_chronicles_6_7_hallucination_detection_faithfulness_gpt4_llamaindex_ragas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - LLM Chronicles: Hallucination Detection with Answer Faithfulness\n",
        "\n",
        "This notebook is part of the **LLM Chronicles** series ([https://llm-chronicles.com/](https://llm-chronicles.com/)) and accompanies the episode on **LLM Hallucinations**, available here: [#6.6: Hallucination Detection and Reduction for RAG systems (RAGAS, Lynx)](https://www.youtube.com/watch?v=xsDNArrmyuo).\n",
        "\n",
        "In this notebook, we'll focus on detecting the faithfulness or groundedness of LLM responses to ensure they are rooted in the source material provided. This means determining if an LLM's answer remains within the scope of the given context without introducing or contradicting information.\n",
        "\n",
        "**Key topics**:\n",
        "\n",
        "- Overview of faithfulness detection methods\n",
        "- Testing LLM responses against various faithfulness frameworks\n",
        "- Applying faithfulness evaluation to detect ungrounded responses\n",
        "\n",
        "This notebook is designed as a practical companion to the YouTube episode, with **hands-on evaluation** of faithfulness controls.\n"
      ],
      "metadata": {
        "id": "uD4hys37P-hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Setup"
      ],
      "metadata": {
        "id": "pus6GHgzYDqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 - Imports and Environment Variables\n",
        "\n",
        "We begin by importing the necessary libraries and setting the OpenAI key environment variable.\n"
      ],
      "metadata": {
        "id": "E68LD1nYXxm9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDKUrn8SVxH1"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade openai langchain langchain_openai llama-index ragas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "pRZZJMnuWDDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Samples\n",
        "The following cells include two sample outputs from a Retrieval-Augmented Generation (RAG) pipeline, each containing:\n",
        "\n",
        "- **User Question**\n",
        "- **Retrieved Context**\n",
        "- **LLM Answer**\n",
        "\n",
        "One sample contains a hallucination, while the other does not. These will be used in testing faithfulness methods.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/6.7%20-%20Lab%20-%20Hallucinations/rag-pipeline.png)\n"
      ],
      "metadata": {
        "id": "HMN9JjwiX7hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_no_hallucination = {\n",
        "  \"question\": \"When was the first super bowl?\",\n",
        "  \"llm_answer\": \"The first superbowl was held on Jan 15, 1967.\",\n",
        "  \"context\": \"The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"\n",
        "}\n",
        "\n",
        "sample_with_hallucination = {\n",
        "  \"question\": \"When was the first super bowl?\",\n",
        "  \"llm_answer\": \"The first superbowl was held on Jan 15, 1989.\",\n",
        "  \"context\": \"The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "L3Dlat7g8vT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Faithfulness Methods\n",
        "\n",
        "The next section covers various methods to evaluate faithfulness in LLM responses."
      ],
      "metadata": {
        "id": "NdDtZyZFYJCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3.1 LLM-as-judge (gpt-4o-mini)\n",
        "This section demonstrates how to use an LLM as a judge to detect faithfulness, using the Lynx prompt ([PatronusAI/Llama-3-Patronus-Lynx-8B-Instruct-v1.1](https://huggingface.co/PatronusAI/Llama-3-Patronus-Lynx-8B-Instruct-v1.1)) with the `gpt-4o-mini` model.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/6.7%20-%20Lab%20-%20Hallucinations/llm-judge.png)\n",
        "\n",
        "This setup allows you to experiment with different LLMs to see how accurately they assess faithfulness. We do not load the Lynx model directly due to high resource requirements (e.g., A100 GPU), but instructions for loading it are available on its Hugging Face page.\n",
        "\n",
        "The next cell contains the full prompt format that guides the LLM in assessing faithfulness.\n"
      ],
      "metadata": {
        "id": "3CdWS5_77irf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "Given the following QUESTION, DOCUMENT and ANSWER you must analyze the provided answer and determine whether it is faithful to the contents of the DOCUMENT. The ANSWER must not offer new information beyond the context provided in the DOCUMENT. The ANSWER also must not contradict information provided in the DOCUMENT. Output your final verdict by strictly following this format: \"PASS\" if the answer is faithful to the DOCUMENT and \"FAIL\" if the answer is not faithful to the DOCUMENT. Show your reasoning.\n",
        "\n",
        "--\n",
        "QUESTION (THIS DOES NOT COUNT AS BACKGROUND INFORMATION):\n",
        "{question}\n",
        "\n",
        "--\n",
        "DOCUMENT:\n",
        "{context}\n",
        "\n",
        "--\n",
        "ANSWER:\n",
        "{answer}\n",
        "\n",
        "--\n",
        "\n",
        "Your output should be in JSON FORMAT with the keys \"REASONING\" and \"SCORE\":\n",
        "{{\"REASONING\": <your reasoning as bullet points>, \"SCORE\": <your final score>}}\n",
        "\"\"\"\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "hallucination_check_prompt = PromptTemplate(\n",
        "    template=PROMPT,\n",
        "    input_variables=[\"question\", \"context\", \"answer\"]\n",
        ")\n",
        "\n",
        "hallucination_check_chain = (\n",
        "    hallucination_check_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "csR5KCn479xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample = sample_no_hallucination\n",
        "\n",
        "hallucination_check_chain.invoke({\n",
        "    \"question\" : test_sample['question'],\n",
        "    \"context\" : test_sample['context'],\n",
        "    \"answer\" : test_sample['llm_answer'],\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "sAkdvJJu8dJX",
        "outputId": "0068534b-c0dc-4a18-8864-b8fcfccbe5d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"REASONING\": [\"The DOCUMENT states that the first AFL–NFL World Championship Game was played on January 15, 1967, which is commonly known as the first Super Bowl.\", \"The ANSWER correctly identifies the date of the first Super Bowl as January 15, 1967.\", \"The ANSWER does not introduce any new information or contradict the information provided in the DOCUMENT.\"], \"SCORE\": \"PASS\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "hallucination_check_chain = (\n",
        "    hallucination_check_prompt\n",
        "    | llm\n",
        "    | JsonOutputParser()\n",
        ")\n",
        "\n",
        "result = hallucination_check_chain.invoke({\n",
        "    \"question\" : test_sample['question'],\n",
        "    \"context\" : test_sample['context'],\n",
        "    \"answer\" : test_sample['llm_answer'],\n",
        "})\n",
        "\n",
        "result.get('SCORE')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MxqKPrOFusBF",
        "outputId": "40c10a21-7b95-405b-fd81-5f4e74ab574e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PASS'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3.1 - Llama-3-Patronus-Lynx-8B-Instruct-v1.1\n",
        "\n",
        "**SKIP UNLESS YOU ATUALLY WANT TO RUN THIS!!!**\n",
        "\n",
        "Lynx: An Open Source Hallucination Evaluation Model\n",
        ": https://arxiv.org/abs/2407.08488\n",
        "\n",
        "https://huggingface.co/PatronusAI/Llama-3-Patronus-Lynx-8B-Instruct-v1.1"
      ],
      "metadata": {
        "id": "gEpm0p5KphXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "model_name = 'PatronusAI/Llama-3-Patronus-Lynx-8B-Instruct-v1.1'\n",
        "# Uncomment to run\n",
        "#pipe = transformers.pipeline(\n",
        "#          \"text-generation\",\n",
        "#          model=model_name,\n",
        "#          max_new_tokens=600,\n",
        "#          device=\"cuda\",\n",
        "#          return_full_text=False\n",
        "#        )\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": hallucination_check_prompt.format(\n",
        "          question = test_sample['question'],\n",
        "          context = test_sample['context'],\n",
        "          answer = test_sample['llm_answer']\n",
        "     )},\n",
        "]\n",
        "\n",
        "#result = pipe(messages)\n",
        "#print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "VgvOphIapLxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Llama-Index Faithfulness\n",
        "\n",
        "[Llama-Index](https://www.llamaindex.ai/) includes a faithfulness evaluation specifically for RAG pipelines, described here: [faithfulness evaluation](https://docs.llamaindex.ai/en/stable/examples/evaluation/faithfulness_eval/). This method uses only the context and the LLM answer (ignores the user question) and works by deploying a simple LLM judge/verifier for faithfulness.\n",
        "\n",
        "Implementation: https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/evaluation/faithfulness.py\n"
      ],
      "metadata": {
        "id": "0o9IttiIgGWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# create llm\n",
        "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
        "\n",
        "# define evaluator\n",
        "llamaindex_evaluator = FaithfulnessEvaluator(llm=llm)"
      ],
      "metadata": {
        "id": "oZEynoPghO0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample = sample_no_hallucination\n",
        "\n",
        "eval_result = llamaindex_evaluator.evaluate(\n",
        "        response=test_sample['llm_answer'],\n",
        "        contexts=[test_sample['context']]\n",
        ")\n",
        "\n",
        "eval_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k-js4_S-yg-",
        "outputId": "e9e9b8a6-ab7b-45ac-fea9-7e768a865a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluationResult(query=None, contexts=['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.'], response='The first superbowl was held on Jan 15, 1967.', passing=True, feedback='YES', score=1.0, pairwise_source=None, invalid_result=False, invalid_reason=None)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 RAGAS Faithfulness\n",
        "\n",
        "In this approach, [RAGAS](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/faithfulness/) evaluates answer faithfulness in two steps:\n",
        "1. The LLM extracts individual claims from the answer.\n",
        "2. Each claim is then verified against the context to see if it’s supported by the context.\n",
        "\n",
        "The faithfulness score is calculated as the ratio of supported claims to total claims, indicating the extent to which the answer is grounded in the context.\n",
        "\n",
        "Implementation: https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_faithfulness.py\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/6.7%20-%20Lab%20-%20Hallucinations/ragas-faithfulness.png)\n"
      ],
      "metadata": {
        "id": "AFv1xdxd62WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "QqIeNRPEWW8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a Single Turn Sample\n",
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import Faithfulness\n",
        "faithfulness_metric = Faithfulness(llm=evaluator_llm)"
      ],
      "metadata": {
        "id": "vxi1qQW5Xrvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample = sample_no_hallucination\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "  user_input= test_sample['question'],\n",
        "  response= test_sample['llm_answer'],\n",
        "  retrieved_contexts=[test_sample['context']]\n",
        ")\n",
        "\n",
        "score = await faithfulness_metric.single_turn_ascore(sample=sample)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mgo9NWUx-Q1X",
        "outputId": "9f37d68d-d495-4efb-9e13-86c34cc47ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 RAGAS with Vectara’s HHEM-2.1-Open\n",
        "RAGAS can also integrate with Vectara's **HHEM-2.1-Open**, a T5 classifier model specifically trained for hallucination detection in LLM-generated text. This model is used in the second step of faithfulness evaluation to verify claims against the context, offering an efficient, open-source solution for real-time applications.\n"
      ],
      "metadata": {
        "id": "oULbTEr4Z9xP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.metrics import FaithfulnesswithHHEM\n",
        "faithfulness_hhem = FaithfulnesswithHHEM(llm=evaluator_llm)\n",
        "\n",
        "score = await faithfulness_hhem.single_turn_ascore(sample=sample)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrI-2sMZaJeW",
        "outputId": "c7db5010-19ee-4704-be01-5c4d5f92a4e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type HHEMv2Config to instantiate a model of type HHEMv2. This is not supported for all configurations of models and can yield errors.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Azure Groundedness Checks\n",
        "\n",
        "The [Azure Groundedness API](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-groundedness?tabs=python) evaluates whether LLM responses are grounded in the user-provided source materials. Although it isn’t explicitly documented, the backend is likely implemented with an LLM judge, similar to Lynx, and returns both an ungroundedness score and reasoning.\n"
      ],
      "metadata": {
        "id": "zKErmiebs9q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import http.client\n",
        "import json\n",
        "\n",
        "def az_check_groundedness(query, answer, context):\n",
        "    key = userdata.get('AZURE_CONTENT_SAFETY_KEY')\n",
        "    endpoint = userdata.get('AZURE_CONTENT_SAFETY_ENDPOINT')\n",
        "\n",
        "    conn = http.client.HTTPSConnection(endpoint)\n",
        "\n",
        "    payload = json.dumps({\n",
        "        \"domain\": \"Generic\",\n",
        "        \"task\": \"QnA\",\n",
        "        \"qna\": {\n",
        "            \"query\": query\n",
        "        },\n",
        "        \"text\": answer,\n",
        "        \"groundingSources\": [context],\n",
        "        \"reasoning\": False\n",
        "    })\n",
        "\n",
        "    headers = {\n",
        "        'Ocp-Apim-Subscription-Key': key,\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    conn.request(\"POST\", \"/contentsafety/text:detectGroundedness?api-version=2024-09-15-preview\", payload, headers)\n",
        "    res = conn.getresponse()\n",
        "    data = res.read()\n",
        "\n",
        "    response_json = json.loads(data.decode(\"utf-8\"))\n",
        "\n",
        "    # Extracting only the required fields\n",
        "    result = {\n",
        "        \"ungroundedDetected\": response_json.get(\"ungroundedDetected\"),\n",
        "        \"ungroundedPercentage\": response_json.get(\"ungroundedPercentage\")\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "K1LGveImtBPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample = sample_no_hallucination\n",
        "\n",
        "az_check_groundedness(\n",
        "    query=test_sample['question'],\n",
        "    answer=test_sample['llm_answer'],\n",
        "    context=test_sample['context']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbfFwVU9yXnq",
        "outputId": "7cd82c11-1bf4-43d0-89f8-f8de9d0c724a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ungroundedDetected': False, 'ungroundedPercentage': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Evaluation with HaluBench\n",
        "\n",
        "[HaluBench](https://huggingface.co/datasets/PatronusAI/HaluBench) is a benchmark for evaluating hallucinations, containing 15,000 context-question-answer triplets annotated for hallucination presence. It includes real-world contexts from domains like finance and medicine. Data is sourced from existing QA datasets such as FinanceBench, PubmedQA, CovidQA, HaluEval, DROP, and RAGTruth.\n"
      ],
      "metadata": {
        "id": "IZ8rETBChm0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(\"hf://datasets/PatronusAI/HaluBench/data/test-00000-of-00001.parquet\")"
      ],
      "metadata": {
        "id": "drdyAzNLkBLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi9Jx5UJkBjJ",
        "outputId": "ea8d4a8f-99e0-490e-e2c9-c2d6e0f10b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14900"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "df.iloc[1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "J1rcsksKkEFY",
        "outputId": "dddb27d2-85fd-48cc-8528-051231f486e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        25985014\n",
              "passage      We investigated the role of surgical ablation targeting the autonomous nervous system during a Cox-Maze IV procedure in the maintenance of sinus rhythm at long-term follow-up.\\nThe patient population consisted of 519 subjects with persistent or long-standing persistent atrial fibrillation (AF) undergoing radiofrequency Maze IV during open heart surgery between January 2006 and July 2013 at three institutions without (Group 1) or with (Group 2) ganglionated plexi (GP) ablation. Recurrence of atrial fibrillation off-antiarrhythmic drugs was the primary outcome. Predictors of AF recurrence were evaluated by means of competing risk regression. Median follow-up was 36.7 months.\\nThe percentage of patients in normal sinus rhythm (NSR) off-antiarrhythmic drugs did not differ between groups (Group 1-75.5%, Group 2-67.8%, p = 0.08). Duration of AF ≥ 38 months (p = 0.01), left atrial diameter ≥ 54 mm (0.001), left atrial area ≥ 33 cm(2) (p = 0.005), absence of connecting lesions (p= 0.04), and absence of right atrial ablation (p<0.001) were independently associated with high incidence of AF recurrence. In contrast the absence of GP ablation was not a significant factor (p = 0.12).\n",
              "question                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Is ganglionated plexi ablation during Maze IV procedure beneficial for postoperative long-term stable sinus rhythm?\n",
              "answer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       No. GP ablation did not prove to be beneficial for postoperative stable NSR. A complete left atrial lesion set and biatrial ablation are advisable for improving rhythm outcomes. Randomized controlled trials are necessary to confirm our findings.\n",
              "label                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         PASS\n",
              "source_ds                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 pubmedQA\n",
              "Name: 1000, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1000</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>25985014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>passage</th>\n",
              "      <td>We investigated the role of surgical ablation targeting the autonomous nervous system during a Cox-Maze IV procedure in the maintenance of sinus rhythm at long-term follow-up.\\nThe patient population consisted of 519 subjects with persistent or long-standing persistent atrial fibrillation (AF) undergoing radiofrequency Maze IV during open heart surgery between January 2006 and July 2013 at three institutions without (Group 1) or with (Group 2) ganglionated plexi (GP) ablation. Recurrence of atrial fibrillation off-antiarrhythmic drugs was the primary outcome. Predictors of AF recurrence were evaluated by means of competing risk regression. Median follow-up was 36.7 months.\\nThe percentage of patients in normal sinus rhythm (NSR) off-antiarrhythmic drugs did not differ between groups (Group 1-75.5%, Group 2-67.8%, p = 0.08). Duration of AF ≥ 38 months (p = 0.01), left atrial diameter ≥ 54 mm (0.001), left atrial area ≥ 33 cm(2) (p = 0.005), absence of connecting lesions (p= 0.04), and absence of right atrial ablation (p&lt;0.001) were independently associated with high incidence of AF recurrence. In contrast the absence of GP ablation was not a significant factor (p = 0.12).</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>question</th>\n",
              "      <td>Is ganglionated plexi ablation during Maze IV procedure beneficial for postoperative long-term stable sinus rhythm?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>answer</th>\n",
              "      <td>No. GP ablation did not prove to be beneficial for postoperative stable NSR. A complete left atrial lesion set and biatrial ablation are advisable for improving rhythm outcomes. Randomized controlled trials are necessary to confirm our findings.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <td>PASS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>source_ds</th>\n",
              "      <td>pubmedQA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 - HaluBenchMini\n",
        "\n",
        "In this notebook, we create a **Halu-Bench-Mini** subset by sampling 100 examples from HaluBench. We'll use this subset to test various faithfulness checks, including:\n",
        "\n",
        "- GPT-4o-mini judge\n",
        "- Llama-Index faithfulness check\n",
        "- RAGAS faithfulness\n",
        "- Azure Groundedness checks"
      ],
      "metadata": {
        "id": "iO14L_JWbISo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the dataset by label\n",
        "pass_items = df[df['label'] == 'PASS']\n",
        "fail_items = df[df['label'] == 'FAIL']\n",
        "\n",
        "# Sample 25 items from each group\n",
        "pass_sample = pass_items.sample(n=50, random_state=42)\n",
        "fail_sample = fail_items.sample(n=50, random_state=42)\n",
        "\n",
        "# Small dataset\n",
        "halu_bench_mini = pd.concat([pass_sample, fail_sample]).reset_index(drop=True)\n",
        "len(halu_bench_mini)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TpDFci2mpOs",
        "outputId": "3931c73c-960a-4d96-fde8-c0b3fbf41a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 - Wrapper Evaluation Function"
      ],
      "metadata": {
        "id": "URXjdeDYlXq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import asyncio\n",
        "\n",
        "def check_hallucination(evaluator_name, question, context, llm_answer):\n",
        "    \"\"\"\n",
        "    Checks if the answer is faithful to the provided context using the specified evaluator.\n",
        "\n",
        "    Parameters:\n",
        "    - evaluator_name: str, the name of the evaluator to use\n",
        "    - question: str, the question or prompt provided\n",
        "    - context: str, the context passage used to verify faithfulness\n",
        "    - llm_answer: str, the answer generated by the LLM\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if the answer is faithful, False if it is a hallucination\n",
        "    \"\"\"\n",
        "\n",
        "    def run_async_func(async_func, *args, **kwargs):\n",
        "        \"\"\"Helper function to run an async function synchronously.\"\"\"\n",
        "        return asyncio.run(async_func(*args, **kwargs))\n",
        "\n",
        "    if evaluator_name == 'llm-judge-gpt4o-mini':\n",
        "        # Invoke hallucination_check_chain and parse JSON response\n",
        "        eval_result = hallucination_check_chain.invoke({\n",
        "            \"question\": question,\n",
        "            \"context\": context,\n",
        "            \"answer\": llm_answer\n",
        "        })\n",
        "        return eval_result.get(\"SCORE\") == \"PASS\"\n",
        "\n",
        "    elif evaluator_name == 'llama-index-faithfulness-gpt4o-mini':\n",
        "      eval_result = llamaindex_evaluator.evaluate(\n",
        "        response=llm_answer,\n",
        "        contexts=[context]\n",
        "      )\n",
        "      return eval_result.passing\n",
        "\n",
        "    elif evaluator_name == 'ragas-faithfulness-gpt4o-mini':\n",
        "        sample = SingleTurnSample(\n",
        "            user_input=question,\n",
        "            response=llm_answer + \".\",\n",
        "            retrieved_contexts=[context]\n",
        "        )\n",
        "        score = run_async_func(faithfulness_metric.single_turn_ascore, sample=sample)\n",
        "        return score >= 0.5\n",
        "\n",
        "    elif evaluator_name == 'azure-groundedness':\n",
        "        eval_result = az_check_groundedness(\n",
        "            query=question,\n",
        "            answer=llm_answer,\n",
        "            context=context\n",
        "        )\n",
        "        return not eval_result['ungroundedDetected']\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unknown evaluator\")\n"
      ],
      "metadata": {
        "id": "Z98Ils9NlcR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of evaluators to test\n",
        "evaluators = [\n",
        "    'llm-judge-gpt4o-mini',\n",
        "    'llama-index-faithfulness-gpt4o-mini',\n",
        "    'ragas-faithfulness-gpt4o-mini',\n",
        "    'azure-groundedness'\n",
        "]\n",
        "\n",
        "# Loop through each evaluator, check hallucination, and print the result\n",
        "for evaluator_name in evaluators:\n",
        "    try:\n",
        "        is_faithful = check_hallucination(\n",
        "            evaluator_name,\n",
        "            sample_no_hallucination['question'],\n",
        "            sample_no_hallucination['context'],\n",
        "            sample_no_hallucination['llm_answer']\n",
        "        )\n",
        "        print(f\"Evaluator: {evaluator_name} - Faithful: {is_faithful}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Evaluator: {evaluator_name} - Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFCSpDI9mzkp",
        "outputId": "614b096f-17e2-480a-a5e3-be6ee4b57bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluator: llm-judge-gpt4o-mini - Faithful: True\n",
            "Evaluator: llama-index-faithfulness-gpt4o-mini - Faithful: True\n",
            "Evaluator: ragas-faithfulness-gpt4o-mini - Faithful: True\n",
            "Evaluator: azure-groundedness - Faithful: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 - Evaluation loop"
      ],
      "metadata": {
        "id": "0ifhOXedl8N1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate(dataset, evaluator_name):\n",
        "    \"\"\"\n",
        "    Evaluates a dataset using a specified evaluator and returns a DataFrame of results.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset: DataFrame, the dataset to evaluate\n",
        "    - evaluator_name: str, the name of the evaluator to use\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame: A DataFrame with the evaluation results\n",
        "    \"\"\"\n",
        "    # Initialize a list to store results\n",
        "    results = []\n",
        "\n",
        "    # Loop through each row in the dataset with tqdm progress bar\n",
        "    for i in tqdm(range(len(dataset)), desc=f\"Evaluating with {evaluator_name}\"):\n",
        "        row = dataset.iloc[i]\n",
        "\n",
        "        # Use check_hallucination to determine faithfulness\n",
        "        is_faithful = check_hallucination(\n",
        "            evaluator_name,\n",
        "            row['question'],\n",
        "            row['passage'],\n",
        "            row['answer']\n",
        "        )\n",
        "\n",
        "        # Check if faithfulness matches the dataset's `label`\n",
        "        is_correct = (is_faithful and row['label'] == 'PASS') or \\\n",
        "                     (not is_faithful and row['label'] == 'FAIL')\n",
        "\n",
        "        # Append result to the list\n",
        "        results.append({\n",
        "            'evaluator': evaluator_name,\n",
        "            'faithful': is_faithful,\n",
        "            'label': row['label'],\n",
        "            'is_correct': is_correct\n",
        "        })\n",
        "\n",
        "    # Convert results to a DataFrame for easy viewing\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "gPvEzQNSoAln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store results DataFrames for each evaluator\n",
        "results_dict = {}\n",
        "\n",
        "# Loop through each evaluator, evaluate the dataset, and store the results\n",
        "for evaluator_name in evaluators:\n",
        "    print(f\"Evaluating with {evaluator_name}...\")\n",
        "    results_df = evaluate(halu_bench_mini, evaluator_name)\n",
        "    results_dict[evaluator_name] = results_df  # Store the result in the dictionary\n",
        "\n",
        "    # Display the first few rows of the result for this evaluator\n",
        "    print(f\"Results for {evaluator_name}:\")\n",
        "    print(results_df.head())\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Optionally, combine all results into a single DataFrame for easier comparison\n",
        "all_results_df = pd.concat(\n",
        "    [df.assign(evaluator=name) for name, df in results_dict.items()],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "# Display the combined results\n",
        "print(\"Combined results across all evaluators:\")\n",
        "print(all_results_df.head())"
      ],
      "metadata": {
        "id": "li222ckSpPNB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea38b816-7322-47b1-bf9f-bdf7c7411815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating with llm-judge-gpt4o-mini...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating with llm-judge-gpt4o-mini: 100%|██████████| 100/100 [02:53<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for llm-judge-gpt4o-mini:\n",
            "              evaluator  faithful label  is_correct\n",
            "0  llm-judge-gpt4o-mini     False  PASS       False\n",
            "1  llm-judge-gpt4o-mini      True  PASS        True\n",
            "2  llm-judge-gpt4o-mini      True  PASS        True\n",
            "3  llm-judge-gpt4o-mini      True  PASS        True\n",
            "4  llm-judge-gpt4o-mini      True  PASS        True\n",
            "\n",
            "==================================================\n",
            "\n",
            "Evaluating with llama-index-faithfulness-gpt4o-mini...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating with llama-index-faithfulness-gpt4o-mini: 100%|██████████| 100/100 [00:36<00:00,  2.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for llama-index-faithfulness-gpt4o-mini:\n",
            "                             evaluator  faithful label  is_correct\n",
            "0  llama-index-faithfulness-gpt4o-mini     False  PASS       False\n",
            "1  llama-index-faithfulness-gpt4o-mini      True  PASS        True\n",
            "2  llama-index-faithfulness-gpt4o-mini      True  PASS        True\n",
            "3  llama-index-faithfulness-gpt4o-mini      True  PASS        True\n",
            "4  llama-index-faithfulness-gpt4o-mini      True  PASS        True\n",
            "\n",
            "==================================================\n",
            "\n",
            "Evaluating with ragas-faithfulness-gpt4o-mini...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating with ragas-faithfulness-gpt4o-mini: 100%|██████████| 100/100 [06:38<00:00,  3.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for ragas-faithfulness-gpt4o-mini:\n",
            "                       evaluator  faithful label  is_correct\n",
            "0  ragas-faithfulness-gpt4o-mini     False  PASS       False\n",
            "1  ragas-faithfulness-gpt4o-mini      True  PASS        True\n",
            "2  ragas-faithfulness-gpt4o-mini     False  PASS       False\n",
            "3  ragas-faithfulness-gpt4o-mini     False  PASS       False\n",
            "4  ragas-faithfulness-gpt4o-mini      True  PASS        True\n",
            "\n",
            "==================================================\n",
            "\n",
            "Evaluating with azure-groundedness...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating with azure-groundedness: 100%|██████████| 100/100 [03:25<00:00,  2.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for azure-groundedness:\n",
            "            evaluator  faithful label  is_correct\n",
            "0  azure-groundedness     False  PASS       False\n",
            "1  azure-groundedness      True  PASS        True\n",
            "2  azure-groundedness      True  PASS        True\n",
            "3  azure-groundedness     False  PASS       False\n",
            "4  azure-groundedness      True  PASS        True\n",
            "\n",
            "==================================================\n",
            "\n",
            "Combined results across all evaluators:\n",
            "              evaluator  faithful label  is_correct\n",
            "0  llm-judge-gpt4o-mini     False  PASS       False\n",
            "1  llm-judge-gpt4o-mini      True  PASS        True\n",
            "2  llm-judge-gpt4o-mini      True  PASS        True\n",
            "3  llm-judge-gpt4o-mini      True  PASS        True\n",
            "4  llm-judge-gpt4o-mini      True  PASS        True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 - Results"
      ],
      "metadata": {
        "id": "-gRtqHfMsVol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store the accuracy (as percentage) for each evaluator\n",
        "accuracy_results = {}\n",
        "\n",
        "# Calculate accuracy for each evaluator based on the `is_correct` column\n",
        "for evaluator_name, results_df in results_dict.items():\n",
        "    # Calculate accuracy as the mean of `is_correct` column, convert to percentage, and round to 2 decimal points\n",
        "    accuracy = round(results_df['is_correct'].mean() * 100, 2)\n",
        "    accuracy_results[evaluator_name] = accuracy\n",
        "\n",
        "# Convert accuracy results to a DataFrame for easy viewing\n",
        "accuracy_df = pd.DataFrame(list(accuracy_results.items()), columns=['Evaluator', 'Accuracy (%)'])\n",
        "\n",
        "# Display the accuracy table\n",
        "print(\"Accuracy of each evaluator on the halu_bench_mini dataset:\")\n",
        "print(accuracy_df)\n"
      ],
      "metadata": {
        "id": "f4-61FqZ1zmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82b4c68-6d4e-44d7-85d7-1145de0ca733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of each evaluator on the halu_bench_mini dataset:\n",
            "                             Evaluator  Accuracy (%)\n",
            "0                 llm-judge-gpt4o-mini          84.0\n",
            "1  llama-index-faithfulness-gpt4o-mini          76.0\n",
            "2        ragas-faithfulness-gpt4o-mini          63.0\n",
            "3                   azure-groundedness          70.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5Ex6VNRrHuv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}