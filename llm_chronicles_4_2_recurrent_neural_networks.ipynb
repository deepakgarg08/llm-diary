{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepakgarg08/llm-diary/blob/main/llm_chronicles_4_2_recurrent_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Chronicles 4.2 - Recurrent Neural Networks\n",
        "\n",
        "In this notebook we'll take a look at how a recurrent layer of a neural network is implemented. Specifically, we'll focus on the recurrence called *hidden-to-hidden*, as this is most commonly used and it's PyTorch's implementation.\n",
        "\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.1%20-%20Lab%20-%20Recurrent%20Neural%20Networks/rnn.png)"
      ],
      "metadata": {
        "id": "JN3nXRlvLgLa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALIltvPcd2G_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. PyTorch RNN layer\n",
        "In this cell, we're creating a simple recurrent layer for a neural network using PyTorch's **nn.RNN** module.\n",
        "\n",
        "Here's a breakdown of the parameters:\n",
        "\n",
        "* **input_size = 2**: This specifies that each input sequence element (or timestep) will have a feature size of 2. In other words, the input tensor to this layer will have a shape like (batch_size, sequence_length, 2) where 2 is the feature size for each element in the sequence.\n",
        "\n",
        "* **hidden_size = 3**: This denotes that the hidden state of the RNN will have a size of 3. The hidden state is the internal state or memory of the RNN, and its size can be thought of as the 'width' or 'capacity' of the RNN's memory at each timestep. When the RNN processes an input, it updates its hidden state based on both the current input and the previous hidden state. This allows the RNN to maintain a form of 'memory' of past inputs in the sequence.\n",
        "\n",
        "* **batch_first=True**: This argument ensures that the expected input tensor shape is (batch_size, sequence_length, input_size). By default, the RNN module in PyTorch expects inputs to have a shape of (sequence_length, batch_size, input_size), but batch_first=True modifies this expectation to be more in line with what's commonly used in other deep learning frameworks and for easier handling."
      ],
      "metadata": {
        "id": "s4HveJxFN5nI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_layer = nn.RNN(input_size = 2, hidden_size=3, batch_first=True)\n",
        "rnn_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqi7tbJ9d9pa",
        "outputId": "42f6f09d-b88c-4a9e-ff36-3046b6f6e685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(2, 3, batch_first=True)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the parameters of this RNN layer\n",
        "w_ih = rnn_layer.weight_ih_l0\n",
        "w_hh = rnn_layer.weight_hh_l0\n",
        "b_ih = rnn_layer.bias_ih_l0\n",
        "b_hh = rnn_layer.bias_hh_l0\n",
        "\n",
        "w_ih, b_ih, w_hh, b_hh"
      ],
      "metadata": {
        "id": "5jPAMERafyPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d661ad23-2e99-4469-d4fb-03370109baa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[ 0.5407, -0.1051],\n",
              "         [ 0.1025,  0.3401],\n",
              "         [-0.1288,  0.4972]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.5645,  0.1087, -0.1245], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[-0.2017, -0.4285, -0.0833],\n",
              "         [ 0.4595,  0.3113, -0.0283],\n",
              "         [-0.5705, -0.0535, -0.1654]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.3875,  0.5334, -0.2906], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a dummy input sequence with 4 time-steps, each having 2 features\n",
        "input_seq = torch.tensor([[1.0]*2,[2.0]*2,[3.0]*2,[4.0]*2])\n",
        "input_seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IILpjcT5hYPj",
        "outputId": "edf591d1-227f-4b11-eca9-5b7942b8c8ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 3.],\n",
              "        [4., 4.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR4wXuaYh0lq",
        "outputId": "352ded54-1170-43f8-cdd1-75b251561cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we unsqueeze to add a batch dimension\n",
        "input_batch = input_seq.unsqueeze(0)\n",
        "input_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANt7UL56Q1b1",
        "outputId": "3f68c3cd-bef3-47eb-b025-eb356f78612c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass\n",
        "output, hh = rnn_layer(input_batch)\n",
        "output, hh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yiORRePhr7i",
        "outputId": "c8833f61-987b-4e1b-aeef-0df580e5fd8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.2529,  0.7949, -0.0466],\n",
              "          [ 0.2971,  0.9556,  0.1416],\n",
              "          [ 0.5706,  0.9837,  0.4186],\n",
              "          [ 0.7589,  0.9947,  0.5448]]], grad_fn=<TransposeBackward1>),\n",
              " tensor([[[0.7589, 0.9947, 0.5448]]], grad_fn=<StackBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. RNN forward pass from scratch\n",
        "\n",
        "Let's now implement the RNN computations from scratch.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.1%20-%20Lab%20-%20Recurrent%20Neural%20Networks/rnn-layer.png)"
      ],
      "metadata": {
        "id": "jJdqaHpWREBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states = [torch.zeros(3)]                                                         # initialize hidden state to zero\n",
        "\n",
        "for t in range(0,4):                                                              # iterate through time steps\n",
        "  print(\"Time step: \", t)\n",
        "  xt = input_seq[t]                                                               # extract first element\n",
        "  ht = torch.matmul(xt, torch.transpose(w_ih, 0, 1)) + b_ih                       # weighted sum of input + bias\n",
        "  ot = ht + torch.matmul(states[t], torch.transpose(w_hh, 0, 1)) + b_hh           # factor in hidden state from previous step\n",
        "  ot = torch.tanh(ot)                                                             # final activation\n",
        "  states.append(ot)\n",
        "  print(\"   Weighted sum with input:\", ht)\n",
        "  print(\"   Output:\", ot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpUwFzL4iIYP",
        "outputId": "0161ccea-562f-4c27-8b04-9d041b9dfcb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time step:  0\n",
            "   Weighted sum with input: tensor([-0.1290,  0.5513,  0.2439], grad_fn=<AddBackward0>)\n",
            "   Output: tensor([ 0.2529,  0.7949, -0.0466], grad_fn=<TanhBackward0>)\n",
            "Time step:  1\n",
            "   Weighted sum with input: tensor([0.3066, 0.9938, 0.6123], grad_fn=<AddBackward0>)\n",
            "   Output: tensor([0.2971, 0.9556, 0.1416], grad_fn=<TanhBackward0>)\n",
            "Time step:  2\n",
            "   Weighted sum with input: tensor([0.7421, 1.4364, 0.9806], grad_fn=<AddBackward0>)\n",
            "   Output: tensor([0.5706, 0.9837, 0.4186], grad_fn=<TanhBackward0>)\n",
            "Time step:  3\n",
            "   Weighted sum with input: tensor([1.1777, 1.8790, 1.3490], grad_fn=<AddBackward0>)\n",
            "   Output: tensor([0.7589, 0.9947, 0.5448], grad_fn=<TanhBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. PyTorch LSTM cell\n",
        "In this cell, we're constructing a Long Short-Term Memory (LSTM) layer using PyTorch's nn.LSTM module. LSTMs are a special kind of recurrent neural network (RNN) that are particularly effective at learning and remembering over long sequences and are less susceptible to the vanishing gradient problem.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.1%20-%20Lab%20-%20Recurrent%20Neural%20Networks/lstm.png)\n",
        "\n",
        "The forward pass through the LSTM returns two items:\n",
        "\n",
        "* **output**: This tensor contains the LSTM's output for each timestep in the input sequence for each batch, exactly as a regular RNN layer.\n",
        "\n",
        "* **(hidden_state, cell_state)**: This is a tuple containing the final hidden and cell states for the LSTM. The hidden state can be thought of as the LSTM's memory of the most recent timestep, while the cell state is a longer-term memory."
      ],
      "metadata": {
        "id": "9uHwOCN0TNfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_layer = nn.LSTM(input_size=2, hidden_size=3, batch_first=True)\n",
        "\n",
        "# Forward pass\n",
        "output, (hidden_state, cell_state) = lstm_layer(input_seq)\n",
        "output, (hidden_state, cell_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CwKQTZrUg35",
        "outputId": "7262ff75-cf45-46cc-c645-3cf605e21d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0439, -0.1310,  0.1267],\n",
              "         [ 0.0249, -0.1867,  0.1401],\n",
              "         [-0.0658, -0.1846,  0.1176],\n",
              "         [-0.1995, -0.1537,  0.0968]], grad_fn=<SqueezeBackward1>),\n",
              " (tensor([[-0.1995, -0.1537,  0.0968]], grad_fn=<SqueezeBackward1>),\n",
              "  tensor([[-0.5042, -0.1580,  0.4641]], grad_fn=<SqueezeBackward1>)))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}