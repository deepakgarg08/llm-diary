{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepakgarg08/llm-diary/blob/main/llm_chronicles_rnn_encoder_decoder_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Translation with RNNs\n",
        "\n",
        "In this notebook we'll implement an encoder/decoder RNN for language translation from English to Italian, similar to the model described in the 2014 paper \"Sequence to Sequence Learning with Neural Networks\" by Ilya Sutskever (OpenAI) et all.\n",
        "\n",
        "https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf"
      ],
      "metadata": {
        "id": "2koIen_kARPa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT1lapS6-IBd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device-independent code\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE"
      ],
      "metadata": {
        "id": "mXxwIY6dgwfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "051a3114-73c6-43d1-de31-c1e20f75281f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Processing dataset\n",
        "\n",
        "For this network, we'll use a distilled version of this \"English_Italian_Sentence_Translation\" dataset from Kaggle (https://www.kaggle.com/datasets/ncsaayali/english-italian-sentence-translation).\n",
        "\n",
        "I have pre-proccessed this dataset by reducing the vocabulary and removing punctuation, so that our small RNN can quickly learn some useful patterns.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.5%20-%20Lab%20-%20Translation/dataset.png)\n"
      ],
      "metadata": {
        "id": "61SL6O9iDczy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/datasets/eng_ita_v2.txt"
      ],
      "metadata": {
        "id": "mnIGIGwiDgvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ffb423-657c-4853-bd71-6d2145795018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-19 17:05:34--  https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/datasets/eng_ita_v2.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7240475 (6.9M) [text/plain]\n",
            "Saving to: ‘eng_ita_v2.txt’\n",
            "\n",
            "\reng_ita_v2.txt        0%[                    ]       0  --.-KB/s               \reng_ita_v2.txt      100%[===================>]   6.90M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-12-19 17:05:34 (162 MB/s) - ‘eng_ita_v2.txt’ saved [7240475/7240475]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to read the file and extract pairs\n",
        "def read_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.read().strip().split('\\n')\n",
        "    pairs = [[s for s in line.split(' -> ')] for line in lines]\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "yCl9WhtNN1TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to text file\n",
        "file_path = 'eng_ita_v2.txt'\n",
        "\n",
        "# Process the data\n",
        "pairs = read_data(file_path)\n",
        "len(pairs)"
      ],
      "metadata": {
        "id": "30vG0l1ZKWrG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f7381c-57ec-46a6-c1f8-f16819bceb07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120746"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.5%20-%20Lab%20-%20Translation/vocab.png)"
      ],
      "metadata": {
        "id": "spSWX6GwKDNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocabulary\n",
        "def tokenize(sentence):\n",
        "  return sentence.lower().split()\n",
        "\n",
        "# Tokenize and build vocabularies\n",
        "def build_vocab(pairs):\n",
        "    eng_vocab = set()\n",
        "    ita_vocab = set()\n",
        "    for eng, ita in pairs:\n",
        "        eng_vocab.update(tokenize(eng))\n",
        "        ita_vocab.update(tokenize(ita))\n",
        "    return eng_vocab, ita_vocab\n",
        "\n",
        "english_vocab, italian_vocab = build_vocab(pairs)\n",
        "\n",
        "# Creating word to integer mapping\n",
        "eng_word2int = {word: i for i, word in enumerate(english_vocab)}\n",
        "ita_word2int = {word: i for i, word in enumerate(italian_vocab)}\n",
        "\n",
        "# Creating integer to word mapping\n",
        "eng_int2word = {i: word for word, i in eng_word2int.items()}\n",
        "ita_int2word = {i: word for word, i in ita_word2int.items()}\n",
        "\n",
        "print('English vocabulary size:', len(english_vocab))\n",
        "print('Italian vocabulary size:', len(italian_vocab))"
      ],
      "metadata": {
        "id": "GkUurywmJ48U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3813611d-c871-4474-ce9d-97c9ca48ef1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocabulary size: 4997\n",
            "Italian vocabulary size: 13673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "eng_example = \"Who are you\"\n",
        "ita_example = \"chi sei tu\"\n",
        "\n",
        "# Encoding\n",
        "eng_encoded = np.array([eng_word2int[word] for word in tokenize(eng_example)], dtype=np.int32)\n",
        "ita_encoded = np.array([ita_word2int[word] for word in tokenize(ita_example)], dtype=np.int32)\n",
        "\n",
        "print('English text encoded:', eng_encoded)\n",
        "print('Italian text encoded:', ita_encoded)\n",
        "\n",
        "# Decoding\n",
        "print('Decoded English:', ' '.join([eng_int2word[i] for i in eng_encoded]))\n",
        "print('Decoded Italian:', ' '.join([ita_int2word[i] for i in ita_encoded]))"
      ],
      "metadata": {
        "id": "6wpIt3jSN5dW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "706a4702-d5f5-4ae9-8228-5101ed7995bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English text encoded: [4492 3186 4986]\n",
            "Italian text encoded: [ 4502  2356 10759]\n",
            "Decoded English: who are you\n",
            "Decoded Italian: chi sei tu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 - Dataset and Dataloader\n",
        "\n",
        "As usual, we'll wrap our dataset into PyTorch Dataset and Dataloder objects, so that we can interact them in a standard way during treaning.\n",
        "\n",
        "We'll also add some special tokens to our dataset. These tokens play crucial roles in the processing and generation of sequences:\n",
        "\n",
        "- **PAD_TOKEN** (<PAD>): This token is used for padding shorter sequences to match the length of the longest sequence in a batch. Padding ensures that all sequences in a batch have the same length, which is a requirement for many models.\n",
        "\n",
        "- **EOS_TOKEN** (<EOS>): The End Of Sequence/Sentence token signifies the end of a sequence. This is particularly important in models that generate text, as it indicates when the model should stop generating further output.\n",
        "\n",
        "- **SOS_TOKEN** (<SOS>): The Start Of Sequence/Sentence token is used to signal the beginning of a new sequence. This is often used in models that generate sequences, as it indicates the start of a new textual output.\n",
        "\n",
        "- **UNK_TOKEN** (<UNK>): The Unknown token is used to represent words that are not in the vocabulary. This is a common practice in NLP applications to handle rare or unknown words that the model may encounter.\n",
        "\n",
        "By incorporating these tokens into our dataset, we can effectively manage sequence lengths and handle special cases in sequence generation and processing. In the next steps, we will integrate these tokens into our dataset preprocessing and model training pipeline."
      ],
      "metadata": {
        "id": "CbzpaLi7Qf9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Special tokens\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "EOS_TOKEN = \"<EOS>\"\n",
        "SOS_TOKEN = \"<SOS>\"\n",
        "UNK_TOKEN = \"<UNK>\"\n",
        "\n",
        "# Update the function to create mappings to include the special tokens\n",
        "def create_mappings(vocab):\n",
        "    vocab = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN] + sorted(vocab)\n",
        "    word2int = {word: i for i, word in enumerate(vocab)}\n",
        "    int2word = {i: word for word, i in word2int.items()}\n",
        "    return word2int, int2word\n",
        "\n",
        "# Update the vocabularies\n",
        "eng_word2int, eng_int2word = create_mappings(english_vocab)\n",
        "ita_word2int, ita_int2word = create_mappings(italian_vocab)"
      ],
      "metadata": {
        "id": "loqdS_5eQkdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, eng_word2int, ita_word2int):\n",
        "        self.pairs = pairs\n",
        "        self.eng_word2int = eng_word2int\n",
        "        self.ita_word2int = ita_word2int\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        eng, ita = self.pairs[idx]\n",
        "        eng_tensor = torch.tensor([self.eng_word2int[word] for word in tokenize(eng)]\n",
        "                                  + [self.eng_word2int[EOS_TOKEN]], dtype=torch.long)\n",
        "        ita_tensor = torch.tensor([self.ita_word2int[word] for word in tokenize(ita)]\n",
        "                                  + [self.ita_word2int[EOS_TOKEN]], dtype=torch.long)\n",
        "        return eng_tensor, ita_tensor\n",
        "\n",
        "# Custom collate function to handle padding\n",
        "def collate_fn(batch):\n",
        "    eng_batch, ita_batch = zip(*batch)\n",
        "    eng_batch_padded = pad_sequence(eng_batch, batch_first=True, padding_value=eng_word2int[PAD_TOKEN])\n",
        "    ita_batch_padded = pad_sequence(ita_batch, batch_first=True, padding_value=ita_word2int[PAD_TOKEN])\n",
        "    return eng_batch_padded, ita_batch_padded"
      ],
      "metadata": {
        "id": "7W0Pmcd5QnkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Create the dataset and DataLoader\n",
        "translation_dataset = TranslationDataset(pairs, eng_word2int, ita_word2int)\n",
        "batch_size = 64\n",
        "translation_dataloader = DataLoader(translation_dataset, batch_size=batch_size,\n",
        "                                    shuffle=True,  drop_last=True, collate_fn=collate_fn)\n",
        "\n",
        "print(\"Translation samples: \", len(translation_dataset))\n",
        "print(\"Translation batches: \", len(translation_dataloader))"
      ],
      "metadata": {
        "id": "5qk88CrMSq6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cfc2a55-73bf-48bf-9e95-d73eccf0fb2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation samples:  120746\n",
            "Translation batches:  1886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: iterating over the DataLoader\n",
        "for eng, ita in translation_dataloader:\n",
        "    print(\"English batch:\", eng)\n",
        "    print(\"Italian batch:\", ita)\n",
        "    break # remove this to iterate over the whole dataset"
      ],
      "metadata": {
        "id": "lr-FTXSG8sq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a13e3f-95b2-47aa-f11e-961676df9f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English batch: tensor([[4848,  253, 4407,  ...,    0,    0,    0],\n",
            "        [4478,  646, 2629,  ...,    0,    0,    0],\n",
            "        [4478, 2395, 2629,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [3825, 1658, 2030,  ...,    0,    0,    0],\n",
            "        [2484, 4392, 2074,  ...,    0,    0,    0],\n",
            "        [4985, 1982, 4467,  ...,    0,    0,    0]])\n",
            "Italian batch: tensor([[ 3264, 12016,  9301,  ...,     0,     0,     0],\n",
            "        [12656,  2347,  7031,  ...,     0,     0,     0],\n",
            "        [12656, 10590,  2331,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [ 5580,  9932,  6453,  ...,     0,     0,     0],\n",
            "        [ 2331,  6122,  6634,  ...,     0,     0,     0],\n",
            "        [ 7774,  8382, 13332,  ...,     0,     0,     0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Encoder / Decoder RNN\n",
        "\n",
        "We'll now implement this delayed sequence-to-sequence model, with two distinct components:\n",
        "\n",
        "- **Encoder**: the first RNN component processes the input sentence and converts it into a fixed-size hidden representation.\n",
        "\n",
        "\n",
        "- **Decoder**: the second RNN component takes the hidden representation from the encoder and produces the translated sentence in the target language.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.5%20-%20Lab%20-%20Translation/encoder-decoder_1.png)"
      ],
      "metadata": {
        "id": "O7Q1BrdKObH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers,\n",
        "                            batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reversing the sequence of indices\n",
        "        x = torch.flip(x, [1])\n",
        "        embedded = self.embedding(x)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return outputs, hidden, cell"
      ],
      "metadata": {
        "id": "_ecbJY18-YzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers,\n",
        "                            batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x)\n",
        "        out, (hidden, cell) = self.lstm(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell"
      ],
      "metadata": {
        "id": "aBB_SwHM-cSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "eng_vocab_size = len(eng_word2int)\n",
        "ita_vocab_size = len(ita_word2int)\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "\n",
        "# Initialize the models\n",
        "encoder = Encoder(eng_vocab_size, embed_size, hidden_size, num_layers).to(DEVICE)\n",
        "decoder = Decoder(ita_vocab_size, embed_size, hidden_size, num_layers).to(DEVICE)"
      ],
      "metadata": {
        "id": "DEpM93wn-hDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Translation (inference)\n",
        "\n",
        "Both the encoder and decoder RNNs will take in a sequence of tokens and project them into embeddings. When it comes to the encoder, we discard the outputs at the individual time steps and only consider the last hidden state which we feed into the decoder. The decoder works in the same way as a regular language model, predicting the Italian translation one word at a time.\n",
        "\n",
        "As for our basic language model, the decoder typically has a linear layer at the end that outputs the logits which give us the probability distribution across the Italian vocabulary of the next word. The only difference with a regular language model is that we use a special SOS as the seed for the first time step. We continue generating words until the model outputs another special token called the “EOS” (end of sentence).\n",
        "\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.5%20-%20Lab%20-%20Translation/encoder-decoder_2.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cIaTMO2jVduQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(encoder, decoder, sentence, eng_word2int, ita_int2word, max_length=15):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    with torch.inference_mode():\n",
        "        # Tokenize and encode the sentence\n",
        "        input_tensor = torch.tensor([eng_word2int[word] for word in tokenize(sentence)]\n",
        "                                    + [eng_word2int[EOS_TOKEN]], dtype=torch.long)\n",
        "        input_tensor = input_tensor.view(1, -1).to(DEVICE)  # batch_first=True\n",
        "\n",
        "        # Pass the input through the encoder\n",
        "        _, encoder_hidden, encoder_cell = encoder(input_tensor)\n",
        "\n",
        "        # Initialize the decoder input with the SOS token\n",
        "        decoder_input = torch.tensor([[eng_word2int[SOS_TOKEN]]], dtype=torch.long)  # SOS\n",
        "        # Initialize the hidden state of the decoder with the encoder's hidden state\n",
        "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
        "\n",
        "        # Decoding the sentence\n",
        "        decoded_words = []\n",
        "        last_word = torch.tensor([[eng_word2int[SOS_TOKEN]]]).to(DEVICE)\n",
        "        for di in range(max_length):\n",
        "            logits, decoder_hidden, decoder_cell = decoder(last_word, decoder_hidden, decoder_cell)\n",
        "            next_token = logits.argmax(dim=1) # greedy\n",
        "            last_word = torch.tensor([[next_token]]).to(DEVICE)\n",
        "            if next_token.item() == ita_word2int[EOS_TOKEN]:\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(ita_int2word.get(next_token.item()))\n",
        "\n",
        "        return ' '.join(decoded_words)"
      ],
      "metadata": {
        "id": "JRwpn5iYVlje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "# s/he likes music, listening to music,\n",
        "# tom left yesterday, they left yesterday\n",
        "# i want to go home now\n",
        "# tom likes chocolate\n",
        "# tom was right about that\n",
        "# tom said he would not come\n",
        "\n",
        "sentence = \"tom said he would not come\"\n",
        "translated_sentence = translate(encoder, decoder, sentence, eng_word2int, ita_int2word)\n",
        "print(\"Translated:\", translated_sentence)"
      ],
      "metadata": {
        "id": "XHW6-6UeVoDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ae6176-70e9-409f-ff6d-95640fb6eefd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated: tom ha detto che non sarebbe venuto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 - Training\n",
        "\n",
        "Let's now implement the training loop, using teacher forcing. **Teacher forcing** is a training method used in the context of sequential models, particularly in language modeling and machine translation. This technique involves using the actual target output from the previous time step as the current input, rather than using the model's prediction from the previous step.\n",
        "\n",
        "During training, this approach accelerates and stabilizes learning by providing the model with the correct context for generating the subsequent part of the sequence. However, while teacher forcing can speed up training and often improve the model's performance on the training data, it can also lead to issues like the model becoming overly reliant on this guidance. This reliance can result in a discrepancy between training and inference phases, as the model may struggle to generate accurate predictions on its own during inference, a phenomenon known as exposure bias.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.5%20-%20Lab%20-%20Translation/training.png)\n",
        "\n",
        "As an optimizer we use **AdamW** (https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html). The AdamW optimizer is an extension of the Adam optimizer, known for combining momentum and RMSProp techniques. The key addition in AdamW is the modification of the weight decay component. Unlike traditional **L2 regularization**, which is applied directly to the loss function, AdamW decouples weight decay from the loss function and directly applies it to the parameter updates. This approach helps in better controlling overfitting by **penalizing large weights**, and it's particularly effective when training deep neural networks, as it allows for more precise and stable optimization of complex models.\n"
      ],
      "metadata": {
        "id": "r2pd1LVITzcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "# Loss Function (exclude padding)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=eng_word2int[PAD_TOKEN])\n",
        "\n",
        "# Optimizers\n",
        "encoder_optimizer = optim.AdamW(encoder.parameters())\n",
        "decoder_optimizer = optim.AdamW(decoder.parameters())\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Training Loop\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (input_tensor, target_tensor) in enumerate(translation_dataloader):\n",
        "        input_tensor, target_tensor = input_tensor.to(DEVICE), target_tensor.to(DEVICE)\n",
        "\n",
        "        # Zero gradients of both optimizers\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        target_length = target_tensor.size(1)\n",
        "\n",
        "        # Encoder\n",
        "        _, encoder_hidden, encoder_cell = encoder(input_tensor)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_input = torch.full((batch_size, 1), eng_word2int[SOS_TOKEN], dtype=torch.long).to(DEVICE)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_cell = encoder_cell\n",
        "\n",
        "        # Randomly select a word index from the target sequence\n",
        "        random_word_index = random.randint(0, target_length - 1)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        for di in range(target_length):\n",
        "            logits, decoder_hidden, decoder_cell  = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
        "            #if di == random_word_index:\n",
        "            #    loss = loss_fn(logits, target_tensor[:, di])\n",
        "            #    break  # Only compute loss for the randomly selected word\n",
        "            loss += loss_fn(logits, target_tensor[:,di])\n",
        "            decoder_input = target_tensor[:, di].reshape(batch_size, 1)  # Teacher forcing\n",
        "\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:  # Print loss every 10 batches\n",
        "            print(f'Epoch {epoch}, Batch {i}, Loss: {loss.item() / target_length:.4f}')\n"
      ],
      "metadata": {
        "id": "qundAzVU-lnz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee0fcc11-8a36-4c99-f183-5a863047b3c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Batch 0, Loss: 9.5280\n",
            "Epoch 0, Batch 100, Loss: 4.4034\n",
            "Epoch 0, Batch 200, Loss: 4.1602\n",
            "Epoch 0, Batch 300, Loss: 4.5719\n",
            "Epoch 0, Batch 400, Loss: 3.5316\n",
            "Epoch 0, Batch 500, Loss: 3.4764\n",
            "Epoch 0, Batch 600, Loss: 3.1723\n",
            "Epoch 0, Batch 700, Loss: 2.8174\n",
            "Epoch 0, Batch 800, Loss: 2.9532\n",
            "Epoch 0, Batch 900, Loss: 2.5940\n",
            "Epoch 0, Batch 1000, Loss: 2.8760\n",
            "Epoch 0, Batch 1100, Loss: 2.6314\n",
            "Epoch 0, Batch 1200, Loss: 2.4217\n",
            "Epoch 0, Batch 1300, Loss: 3.2909\n",
            "Epoch 0, Batch 1400, Loss: 2.4033\n",
            "Epoch 0, Batch 1500, Loss: 2.5604\n",
            "Epoch 0, Batch 1600, Loss: 2.1422\n",
            "Epoch 0, Batch 1700, Loss: 2.2601\n",
            "Epoch 0, Batch 1800, Loss: 3.0945\n",
            "Epoch 1, Batch 0, Loss: 0.7630\n",
            "Epoch 1, Batch 100, Loss: 1.4685\n",
            "Epoch 1, Batch 200, Loss: 1.6303\n",
            "Epoch 1, Batch 300, Loss: 1.3961\n",
            "Epoch 1, Batch 400, Loss: 1.3776\n",
            "Epoch 1, Batch 500, Loss: 1.4368\n",
            "Epoch 1, Batch 600, Loss: 1.2745\n",
            "Epoch 1, Batch 700, Loss: 1.0998\n",
            "Epoch 1, Batch 800, Loss: 1.1950\n",
            "Epoch 1, Batch 900, Loss: 1.4213\n",
            "Epoch 1, Batch 1000, Loss: 1.2337\n",
            "Epoch 1, Batch 1100, Loss: 1.4407\n",
            "Epoch 1, Batch 1200, Loss: 1.5391\n",
            "Epoch 1, Batch 1300, Loss: 1.4938\n",
            "Epoch 1, Batch 1400, Loss: 1.9401\n",
            "Epoch 1, Batch 1500, Loss: 1.4487\n",
            "Epoch 1, Batch 1600, Loss: 1.5078\n",
            "Epoch 1, Batch 1700, Loss: 1.4365\n",
            "Epoch 1, Batch 1800, Loss: 1.3072\n",
            "Epoch 2, Batch 0, Loss: 0.7768\n",
            "Epoch 2, Batch 100, Loss: 0.8637\n",
            "Epoch 2, Batch 200, Loss: 0.9350\n",
            "Epoch 2, Batch 300, Loss: 0.8297\n",
            "Epoch 2, Batch 400, Loss: 0.9276\n",
            "Epoch 2, Batch 500, Loss: 0.5788\n",
            "Epoch 2, Batch 600, Loss: 0.6000\n",
            "Epoch 2, Batch 700, Loss: 0.8687\n",
            "Epoch 2, Batch 800, Loss: 0.7284\n",
            "Epoch 2, Batch 900, Loss: 0.7221\n",
            "Epoch 2, Batch 1000, Loss: 0.7815\n",
            "Epoch 2, Batch 1100, Loss: 0.8799\n",
            "Epoch 2, Batch 1200, Loss: 0.6038\n",
            "Epoch 2, Batch 1300, Loss: 0.7242\n",
            "Epoch 2, Batch 1400, Loss: 0.6079\n",
            "Epoch 2, Batch 1500, Loss: 0.7963\n",
            "Epoch 2, Batch 1600, Loss: 0.6455\n",
            "Epoch 2, Batch 1700, Loss: 0.7115\n",
            "Epoch 2, Batch 1800, Loss: 0.9264\n",
            "Epoch 3, Batch 0, Loss: 0.4180\n",
            "Epoch 3, Batch 100, Loss: 0.4715\n",
            "Epoch 3, Batch 200, Loss: 0.4359\n",
            "Epoch 3, Batch 300, Loss: 0.5142\n",
            "Epoch 3, Batch 400, Loss: 0.7775\n",
            "Epoch 3, Batch 500, Loss: 0.4505\n",
            "Epoch 3, Batch 600, Loss: 0.6023\n",
            "Epoch 3, Batch 700, Loss: 0.5065\n",
            "Epoch 3, Batch 800, Loss: 0.5423\n",
            "Epoch 3, Batch 900, Loss: 0.6395\n",
            "Epoch 3, Batch 1000, Loss: 0.5279\n",
            "Epoch 3, Batch 1100, Loss: 0.4578\n",
            "Epoch 3, Batch 1200, Loss: 0.5363\n",
            "Epoch 3, Batch 1300, Loss: 0.5504\n",
            "Epoch 3, Batch 1400, Loss: 0.5969\n",
            "Epoch 3, Batch 1500, Loss: 0.4341\n",
            "Epoch 3, Batch 1600, Loss: 0.5617\n",
            "Epoch 3, Batch 1700, Loss: 0.6257\n",
            "Epoch 3, Batch 1800, Loss: 0.7001\n",
            "Epoch 4, Batch 0, Loss: 0.2840\n",
            "Epoch 4, Batch 100, Loss: 0.3723\n",
            "Epoch 4, Batch 200, Loss: 0.3715\n",
            "Epoch 4, Batch 300, Loss: 0.3450\n",
            "Epoch 4, Batch 400, Loss: 0.4773\n",
            "Epoch 4, Batch 500, Loss: 0.4442\n",
            "Epoch 4, Batch 600, Loss: 0.4732\n",
            "Epoch 4, Batch 700, Loss: 0.2950\n",
            "Epoch 4, Batch 800, Loss: 0.3816\n",
            "Epoch 4, Batch 900, Loss: 0.4702\n",
            "Epoch 4, Batch 1000, Loss: 0.3617\n",
            "Epoch 4, Batch 1100, Loss: 0.3904\n",
            "Epoch 4, Batch 1200, Loss: 0.3368\n",
            "Epoch 4, Batch 1300, Loss: 0.3070\n",
            "Epoch 4, Batch 1400, Loss: 0.2739\n",
            "Epoch 4, Batch 1500, Loss: 0.3317\n",
            "Epoch 4, Batch 1600, Loss: 0.3397\n",
            "Epoch 4, Batch 1700, Loss: 0.3867\n",
            "Epoch 4, Batch 1800, Loss: 0.3689\n",
            "Epoch 5, Batch 0, Loss: 0.3057\n",
            "Epoch 5, Batch 100, Loss: 0.2519\n",
            "Epoch 5, Batch 200, Loss: 0.2630\n",
            "Epoch 5, Batch 300, Loss: 0.2190\n",
            "Epoch 5, Batch 400, Loss: 0.2046\n",
            "Epoch 5, Batch 500, Loss: 0.2433\n",
            "Epoch 5, Batch 600, Loss: 0.2819\n",
            "Epoch 5, Batch 700, Loss: 0.2188\n",
            "Epoch 5, Batch 800, Loss: 0.2940\n",
            "Epoch 5, Batch 900, Loss: 0.2637\n",
            "Epoch 5, Batch 1000, Loss: 0.2713\n",
            "Epoch 5, Batch 1100, Loss: 0.3057\n",
            "Epoch 5, Batch 1200, Loss: 0.2530\n",
            "Epoch 5, Batch 1300, Loss: 0.2768\n",
            "Epoch 5, Batch 1400, Loss: 0.2800\n",
            "Epoch 5, Batch 1500, Loss: 0.3324\n",
            "Epoch 5, Batch 1600, Loss: 0.2593\n",
            "Epoch 5, Batch 1700, Loss: 0.2339\n",
            "Epoch 5, Batch 1800, Loss: 0.4169\n",
            "Epoch 6, Batch 0, Loss: 0.1667\n",
            "Epoch 6, Batch 100, Loss: 0.1342\n",
            "Epoch 6, Batch 200, Loss: 0.3881\n",
            "Epoch 6, Batch 300, Loss: 0.2618\n",
            "Epoch 6, Batch 400, Loss: 0.2900\n",
            "Epoch 6, Batch 500, Loss: 0.1983\n",
            "Epoch 6, Batch 600, Loss: 0.2939\n",
            "Epoch 6, Batch 700, Loss: 0.1718\n",
            "Epoch 6, Batch 800, Loss: 0.2356\n",
            "Epoch 6, Batch 900, Loss: 0.5164\n",
            "Epoch 6, Batch 1000, Loss: 0.1671\n",
            "Epoch 6, Batch 1100, Loss: 0.2842\n",
            "Epoch 6, Batch 1200, Loss: 0.2644\n",
            "Epoch 6, Batch 1300, Loss: 0.3090\n",
            "Epoch 6, Batch 1400, Loss: 0.2367\n",
            "Epoch 6, Batch 1500, Loss: 0.2370\n",
            "Epoch 6, Batch 1600, Loss: 0.2672\n",
            "Epoch 6, Batch 1700, Loss: 0.3106\n",
            "Epoch 6, Batch 1800, Loss: 0.2452\n",
            "Epoch 7, Batch 0, Loss: 0.1036\n",
            "Epoch 7, Batch 100, Loss: 0.1361\n",
            "Epoch 7, Batch 200, Loss: 0.1961\n",
            "Epoch 7, Batch 300, Loss: 0.1152\n",
            "Epoch 7, Batch 400, Loss: 0.1200\n",
            "Epoch 7, Batch 500, Loss: 0.1001\n",
            "Epoch 7, Batch 600, Loss: 0.1173\n",
            "Epoch 7, Batch 700, Loss: 0.1759\n",
            "Epoch 7, Batch 800, Loss: 0.1517\n",
            "Epoch 7, Batch 900, Loss: 0.1761\n",
            "Epoch 7, Batch 1000, Loss: 0.1870\n",
            "Epoch 7, Batch 1100, Loss: 0.1737\n",
            "Epoch 7, Batch 1200, Loss: 0.1505\n",
            "Epoch 7, Batch 1300, Loss: 0.2062\n",
            "Epoch 7, Batch 1400, Loss: 0.2711\n",
            "Epoch 7, Batch 1500, Loss: 0.1935\n",
            "Epoch 7, Batch 1600, Loss: 0.1722\n",
            "Epoch 7, Batch 1700, Loss: 0.1842\n",
            "Epoch 7, Batch 1800, Loss: 0.1401\n",
            "Epoch 8, Batch 0, Loss: 0.1353\n",
            "Epoch 8, Batch 100, Loss: 0.1155\n",
            "Epoch 8, Batch 200, Loss: 0.0543\n",
            "Epoch 8, Batch 300, Loss: 0.1032\n",
            "Epoch 8, Batch 400, Loss: 0.0744\n",
            "Epoch 8, Batch 500, Loss: 0.1153\n",
            "Epoch 8, Batch 600, Loss: 0.0934\n",
            "Epoch 8, Batch 700, Loss: 0.1045\n",
            "Epoch 8, Batch 800, Loss: 0.1286\n",
            "Epoch 8, Batch 900, Loss: 0.0952\n",
            "Epoch 8, Batch 1000, Loss: 0.1424\n",
            "Epoch 8, Batch 1100, Loss: 0.1219\n",
            "Epoch 8, Batch 1200, Loss: 0.0958\n",
            "Epoch 8, Batch 1300, Loss: 0.1527\n",
            "Epoch 8, Batch 1400, Loss: 0.1212\n",
            "Epoch 8, Batch 1500, Loss: 0.2053\n",
            "Epoch 8, Batch 1600, Loss: 0.2981\n",
            "Epoch 8, Batch 1700, Loss: 0.1465\n",
            "Epoch 8, Batch 1800, Loss: 0.1561\n",
            "Epoch 9, Batch 0, Loss: 0.0266\n",
            "Epoch 9, Batch 100, Loss: 0.2095\n",
            "Epoch 9, Batch 200, Loss: 0.1382\n",
            "Epoch 9, Batch 300, Loss: 0.5411\n",
            "Epoch 9, Batch 400, Loss: 0.1060\n",
            "Epoch 9, Batch 500, Loss: 0.1437\n",
            "Epoch 9, Batch 600, Loss: 0.1757\n",
            "Epoch 9, Batch 700, Loss: 0.0943\n",
            "Epoch 9, Batch 800, Loss: 0.0580\n",
            "Epoch 9, Batch 900, Loss: 0.0594\n",
            "Epoch 9, Batch 1000, Loss: 0.1253\n",
            "Epoch 9, Batch 1100, Loss: 0.1197\n",
            "Epoch 9, Batch 1200, Loss: 0.1377\n",
            "Epoch 9, Batch 1300, Loss: 0.1581\n",
            "Epoch 9, Batch 1400, Loss: 0.0819\n",
            "Epoch 9, Batch 1500, Loss: 0.1705\n",
            "Epoch 9, Batch 1600, Loss: 0.1427\n",
            "Epoch 9, Batch 1700, Loss: 0.0934\n",
            "Epoch 9, Batch 1800, Loss: 0.1339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Bi-directional RNN\n",
        "\n",
        "A possible optimization is using a bi-directional RNN for the encoder. In this setup, the model processes the input sequence in both the original and reverse order. This results in two sets of hidden states for each part of the sequence, giving the model a fuller understanding of the sentence context from both directions, which can lead to more accurate translations.\n",
        "\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.5%20-%20Lab%20-%20Translation/bi-directional.png)"
      ],
      "metadata": {
        "id": "ZoFHIMK4P8Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers,\n",
        "                            batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # concatenate hidden states of the bi-directional RNN layer\n",
        "        hidden = torch.cat((hidden[0,:,:], hidden[1,:,:]), dim=1).unsqueeze(0)\n",
        "        cell = torch.cat((cell[0,:,:], cell[1,:,:]), dim=1).unsqueeze(0)\n",
        "\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x)\n",
        "        out, (hidden, cell) = self.lstm(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell"
      ],
      "metadata": {
        "id": "ZfAHFh8AOjGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "eng_vocab_size = len(eng_word2int)\n",
        "ita_vocab_size = len(ita_word2int)\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "\n",
        "# Initialize the models\n",
        "encoder = Encoder(eng_vocab_size, embed_size, hidden_size, num_layers).to(DEVICE)\n",
        "decoder = Decoder(ita_vocab_size, embed_size, hidden_size*2, num_layers).to(DEVICE)"
      ],
      "metadata": {
        "id": "1DqbbkqDO8oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3JpPtslXXL8s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}