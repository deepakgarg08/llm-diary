{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepakgarg08/llm-diary/blob/main/llm_chronicles_4_4_word_level_rnn_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Chronicles 4.4: Word-Level Language Model RNN\n",
        "\n",
        "In this lab we'll build a word-level language model using RNNs and LSTM cells.\n",
        "\n",
        "Code based on this character-level RNN from Sebastian Raschka's book \"Machine Learning with PyTorch and Scikit-Learn\": https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part3.ipynb\n"
      ],
      "metadata": {
        "id": "jyIt_L2irLma"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C29dFE5GQTaw"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import requests\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load dataset\n",
        "\n",
        "We'll train our language model on a dataset made of fairy tales. The reasoning for choosing such a dataset is that fairy tales tend to use simpler language with a limited vocabulary, so it'll be easier for our model to learn patterns.\n",
        "\n",
        "I have further cleaned the dataset this way:\n",
        "\n",
        "- I only included sentences that use the top 5000 words, this ensures the vocabulary is limited and words are repeated often throughout the text.\n",
        "- I removed all punctuation except periods.\n",
        "- I removed all sentences that contained quoted speech, such as: She asked: \"What time is it?\". This makes sure sentence structure is simpler.\n",
        "\n",
        "Scroll to the end of this notebook to see the code used to clean-up the dataset."
      ],
      "metadata": {
        "id": "ZAtI-MtPM2f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.4%20-%20Lab%20-%20Word-Level%20RNN/fairy_tales_cleaned_most_common_5000_words.txt -O dataset.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lPIvO2ORIkL",
        "outputId": "58f2c641-4f1e-4e5f-ce7c-bb395b79f298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-29 08:34:39--  https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.4%20-%20Lab%20-%20Word-Level%20RNN/fairy_tales_cleaned_most_common_5000_words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2751228 (2.6M) [text/plain]\n",
            "Saving to: ‘dataset.txt’\n",
            "\n",
            "dataset.txt         100%[===================>]   2.62M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-05-29 08:34:40 (55.9 MB/s) - ‘dataset.txt’ saved [2751228/2751228]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Reading and processing text\n",
        "with open('dataset.txt', 'r', encoding=\"utf8\") as fp:\n",
        "    text=fp.read()\n",
        "\n",
        "print('Total Length (characters):', len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9lu7JW-SDOW",
        "outputId": "8f581bb6-34ac-46af-9df1-f4ab0e9e7ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Length (characters): 2751174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word-level tokenization\n",
        "\n",
        "For this language model, we'll use word-level tokenization. We'll also include the period as a token, which allows us to separate sentences.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.4%20-%20Lab%20-%20Word-Level%20RNN/tokens.png)\n"
      ],
      "metadata": {
        "id": "4P7gQquFxCIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(doc):\n",
        "    # Exclude period from the punctuation list\n",
        "    punctuation_to_remove = string.punctuation.replace('.', '')\n",
        "\n",
        "    # Create translation table that removes specified punctuation except period\n",
        "    table = str.maketrans('', '', punctuation_to_remove)\n",
        "\n",
        "    tokens = doc.split()\n",
        "    # Further split tokens by period and keep periods as separate tokens\n",
        "    split_tokens = []\n",
        "    for token in tokens:\n",
        "        split_tokens.extend(token.replace('.', ' .').split())\n",
        "\n",
        "    tokens = [w.translate(table) for w in split_tokens]\n",
        "    tokens = [word for word in tokens if word.isalpha() or word == '.']\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "yZr9XWMH7CtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "tokens = tokenize(text)\n",
        "print(tokens[:100])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXMGRtzHTMXR",
        "outputId": "2fd7059b-1262-42a4-ca1e-d9f70740089b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'happy', 'prince', '.', 'high', 'above', 'the', 'city', 'on', 'a', 'tall', 'column', 'stood', 'the', 'statue', 'of', 'the', 'happy', 'prince', '.', 'he', 'was', 'very', 'much', 'admired', 'indeed', '.', 'one', 'night', 'there', 'flew', 'over', 'the', 'city', 'a', 'little', 'swallow', '.', 'then', 'when', 'the', 'autumn', 'came', 'they', 'all', 'flew', 'away', '.', 'what', 'did', 'he', 'see', 'the', 'eyes', 'of', 'the', 'happy', 'prince', 'were', 'filled', 'with', 'tears', 'and', 'tears', 'were', 'running', 'down', 'his', 'golden', 'cheeks', '.', 'his', 'face', 'was', 'so', 'beautiful', 'in', 'the', 'moonlight', 'that', 'the', 'little', 'swallow', 'was', 'filled', 'with', 'pity', '.', 'round', 'the', 'garden', 'ran', 'a', 'very', 'lofty', 'wall', 'but', 'i', 'never', 'cared']\n",
            "Total Tokens: 584955\n",
            "Unique Tokens: 5371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create vocabulary\n",
        "\n",
        "We now need to assign each word in the vocabulary to a unique index, which will later be one-hote encoded when we feed it to the model.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.4%20-%20Lab%20-%20Word-Level%20RNN/vocabulary.png)\n"
      ],
      "metadata": {
        "id": "Kxw1FH6vNOrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = sorted(set(tokens))\n",
        "len(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8njXvuIqUcu8",
        "outputId": "40610377-1627-494b-93bb-0fb2a0c3fe47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5371"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2int = {word:i for i,word in enumerate(vocabulary)}\n",
        "word_array = np.array(vocabulary)\n",
        "\n",
        "text_encoded = np.array(\n",
        "    [word2int[word] for word in tokens],\n",
        "    dtype=np.int32)\n",
        "\n",
        "print('Text encoded shape: ', text_encoded.shape)\n",
        "\n",
        "print(\"Tokens ==> \", tokens[:20], '\\nEncoding ==> ', text_encoded[:20])\n",
        "print(text_encoded[0:20], ' == Reverse  ==> ', ' '.join(word_array[text_encoded[:20]]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_Pat935UtZc",
        "outputId": "019acbe7-b69d-4dc8-cc0f-70f62cf5fa16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text encoded shape:  (584955,)\n",
            "Tokens ==>  ['the', 'happy', 'prince', '.', 'high', 'above', 'the', 'city', 'on', 'a', 'tall', 'column', 'stood', 'the', 'statue', 'of', 'the', 'happy', 'prince', '.'] \n",
            "Encoding ==>  [4735 2109 3621    0 2212    7 4735  813 3259    1 4681  882 4518 4735\n",
            " 4482 3236 4735 2109 3621    0]\n",
            "[4735 2109 3621    0 2212    7 4735  813 3259    1 4681  882 4518 4735\n",
            " 4482 3236 4735 2109 3621    0]  == Reverse  ==>  the happy prince . high above the city on a tall column stood the statue of the happy prince .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Prepare pairs for self-supervised training\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.4%20-%20Lab%20-%20Word-Level%20RNN/pairs.png)\n"
      ],
      "metadata": {
        "id": "uBpL6QxwNSpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 50\n",
        "chunk_size = seq_length + 1\n",
        "\n",
        "text_chunks = [text_encoded[i:i+chunk_size]\n",
        "               for i in range(len(text_encoded)-chunk_size+1)]\n",
        "\n",
        "for seq in text_chunks[:1]:\n",
        "    input_seq = seq[:seq_length]\n",
        "    target = seq[seq_length]\n",
        "    print(input_seq, ' -> ', target)\n",
        "    print(repr(' '.join(word_array[input_seq])),\n",
        "          ' -> ', repr(''.join(word_array[target])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VPU5xiMWgkC",
        "outputId": "6718b4a6-8608-42dc-9a74-b73da020e834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4735 2109 3621    0 2212    7 4735  813 3259    1 4681  882 4518 4735\n",
            " 4482 3236 4735 2109 3621    0 2149 5114 5050 3073   50 2371    0 3261\n",
            " 3165 4743 1737 3297 4735  813    1 2733 4633    0 4742 5188 4735  289\n",
            "  660 4751  116 1737  296    0 5179 1214]  ->  2149\n",
            "'the happy prince . high above the city on a tall column stood the statue of the happy prince . he was very much admired indeed . one night there flew over the city a little swallow . then when the autumn came they all flew away . what did'  ->  'he'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
      ],
      "metadata": {
        "id": "L_yAX_bXW1hP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc6180f-0b77-4d62-aa5a-c9daf2976b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-35e90659b81a>:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (seq, target) in enumerate(seq_dataset):\n",
        "    print(' Input (x):', repr(' '.join(word_array[seq])))\n",
        "    print('Target (y):', repr(' '.join(word_array[target])))\n",
        "    print()\n",
        "    if i == 1:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDuRdVo_W-Sn",
        "outputId": "c29540bf-c29a-4cc4-b39d-8475e53a1476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Input (x): 'the happy prince . high above the city on a tall column stood the statue of the happy prince . he was very much admired indeed . one night there flew over the city a little swallow . then when the autumn came they all flew away . what did'\n",
            "Target (y): 'happy prince . high above the city on a tall column stood the statue of the happy prince . he was very much admired indeed . one night there flew over the city a little swallow . then when the autumn came they all flew away . what did he'\n",
            "\n",
            " Input (x): 'happy prince . high above the city on a tall column stood the statue of the happy prince . he was very much admired indeed . one night there flew over the city a little swallow . then when the autumn came they all flew away . what did he'\n",
            "Target (y): 'prince . high above the city on a tall column stood the statue of the happy prince . he was very much admired indeed . one night there flew over the city a little swallow . then when the autumn came they all flew away . what did he see'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "M9u2CJ8vXYUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Create model\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.4%20-%20Lab%20-%20Word-Level%20RNN/model.png)\n"
      ],
      "metadata": {
        "id": "JtfDPGdkNdST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device-independent code\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivkkYgg9XJxx",
        "outputId": "2f2a7e1c-c7d2-48cd-a420-bca180d9196b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden.to(DEVICE), cell.to(DEVICE)\n",
        "\n",
        "vocab_size = len(word_array)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model = model.to(DEVICE)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7G5-FguXeqO",
        "outputId": "3551772c-1780-48a0-87c8-5ebaf616c4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(5371, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=5371, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Train model"
      ],
      "metadata": {
        "id": "TwHO-H-_NiI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "num_epochs = 15000\n",
        "\n",
        "model.to(DEVICE)\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    hidden, cell = model.init_hidden(batch_size)\n",
        "    seq_batch, target_batch = next(iter(seq_dl))\n",
        "    seq_batch = seq_batch.to(DEVICE)\n",
        "    target_batch = target_batch.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    for w in range(seq_length):\n",
        "        pred, hidden, cell = model(seq_batch[:, w], hidden, cell)\n",
        "        loss += loss_fn(pred, target_batch[:, w])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss = loss.item()/seq_length\n",
        "    if epoch % 500 == 0:\n",
        "        print(f'Epoch {epoch} loss: {loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "dzKluonDXtc_",
        "outputId": "b11389a4-b4af-4fe2-edb2-317ae499f3e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 8.5946\n",
            "Epoch 500 loss: 4.0265\n",
            "Epoch 1000 loss: 3.2586\n",
            "Epoch 1500 loss: 2.8442\n",
            "Epoch 2000 loss: 2.6742\n",
            "Epoch 2500 loss: 2.5651\n",
            "Epoch 3000 loss: 2.2923\n",
            "Epoch 3500 loss: 2.2500\n",
            "Epoch 4000 loss: 2.0766\n",
            "Epoch 4500 loss: 2.1702\n",
            "Epoch 5000 loss: 2.0528\n",
            "Epoch 5500 loss: 2.0661\n",
            "Epoch 6000 loss: 1.9789\n",
            "Epoch 6500 loss: 1.9447\n",
            "Epoch 7000 loss: 2.0815\n",
            "Epoch 7500 loss: 1.9007\n",
            "Epoch 8000 loss: 2.1281\n",
            "Epoch 8500 loss: 2.1102\n",
            "Epoch 9000 loss: 2.0661\n",
            "Epoch 9500 loss: 1.9980\n",
            "Epoch 10000 loss: 2.0684\n",
            "Epoch 10500 loss: 2.1574\n",
            "Epoch 11000 loss: 2.1624\n",
            "Epoch 11500 loss: 2.0173\n",
            "Epoch 12000 loss: 2.0622\n",
            "Epoch 12500 loss: 2.0978\n",
            "Epoch 13000 loss: 2.2316\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4b7b17c2adbc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Text Generation\n",
        "\n",
        "## 6.1 Temperature and Top-P Sampling\n",
        "\n",
        "We typically don't want to always pick the word with the highest probability as the next token, as our outputs would become very predictable and often repetitive. Instead, we want our model to be creative and generate diverse outputs.\n",
        "\n",
        "Two common strategies combined together to control the model's \"creativity\" and predictability are:\n",
        "\n",
        "- **Top-p Sampling**: here we sample from the top predictions whose combined probability does not exceed the value p.\n",
        "\n",
        "- **Temperature**:  this directly impacts the probability distribution of the upcoming token. Think of it as tweaking the sharpness of this distribution.When the temperature is set to a value less than one, the softmax probability distribution becomes sharp. On the other hand, a higher temperature spreads out the probability distribution, making it flatter.\n",
        "\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.4%20-%20Lab%20-%20Word-Level%20RNN/topp-temperature.png)\n"
      ],
      "metadata": {
        "id": "Hi3kiO91NlN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_p_sampling(logits, temperature=1.0, top_p=0.9):\n",
        "    # Ensure logits are a PyTorch tensor and move to DEVICE\n",
        "\n",
        "    # Apply temperature scaling\n",
        "    scaled_logits = logits / temperature\n",
        "\n",
        "    # Convert logits to probabilities using softmax\n",
        "    probabilities = torch.softmax(scaled_logits, dim=-1)\n",
        "\n",
        "    # Sort probabilities and compute cumulative sum\n",
        "    sorted_indices = torch.argsort(probabilities, descending=True)\n",
        "    sorted_probabilities = probabilities[sorted_indices]\n",
        "    cumulative_probabilities = torch.cumsum(sorted_probabilities, dim=-1)\n",
        "\n",
        "    # Apply top-p filtering\n",
        "    indices_to_keep = cumulative_probabilities <= top_p\n",
        "    truncated_probabilities = sorted_probabilities[indices_to_keep]\n",
        "\n",
        "    # Rescale the probabilities\n",
        "    truncated_probabilities /= torch.sum(truncated_probabilities)\n",
        "\n",
        "    # Convert to numpy arrays for random choice\n",
        "    truncated_probabilities = truncated_probabilities.cpu().numpy()\n",
        "    sorted_indices = sorted_indices.cpu().numpy()\n",
        "    indices_to_keep = indices_to_keep.cpu().numpy()\n",
        "\n",
        "    # Sample from the truncated distribution\n",
        "    if not indices_to_keep.any():\n",
        "        # Handle the empty case - for example, using regular sampling without top-p\n",
        "        probabilities = torch.softmax(logits / temperature, dim=-1)\n",
        "        next_word_index = torch.multinomial(probabilities, 1).item()\n",
        "    else:\n",
        "        # Existing sampling process\n",
        "        next_word_index = np.random.choice(sorted_indices[indices_to_keep], p=truncated_probabilities)\n",
        "\n",
        "    return torch.tensor(next_word_index).to(DEVICE)\n"
      ],
      "metadata": {
        "id": "9q6nitiSFQwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating text\n",
        "\n",
        "We begin by inputting an initial word or phrase. This is our 'seed' for text generation. The model then looks at this input to predict the next token.\n",
        "The predicted word is then fed back into the model as the next input. The model then uses this new input to predict yet another word or character. This process creates a feedback loop, allowing the model to generate continuous sequences of text, word by word or character by character.\n",
        "\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.4%20-%20Lab%20-%20Word-Level%20RNN/text-generation.png)\n"
      ],
      "metadata": {
        "id": "ybIWGAbmx-29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, seed_str,\n",
        "           len_generated_text=50,\n",
        "           temperature=1, top_p=0.95):\n",
        "\n",
        "    seed_tokens = tokenize(seed_str)\n",
        "\n",
        "    encoded_input = torch.tensor([word2int[t] for t in seed_tokens])\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1)).to(DEVICE)\n",
        "\n",
        "    generated_str = seed_str\n",
        "\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      hidden, cell = model.init_hidden(1)\n",
        "      hidden = hidden.to(DEVICE)\n",
        "      cell = cell.to(DEVICE)\n",
        "      for w in range(len(seed_tokens)-1):\n",
        "          _, hidden, cell = model(encoded_input[:, w].view(1), hidden, cell)\n",
        "\n",
        "      last_word = encoded_input[:, -1]\n",
        "      for i in range(len_generated_text):\n",
        "          logits, hidden, cell = model(last_word.view(1), hidden, cell)\n",
        "          logits = torch.squeeze(logits, 0)\n",
        "          last_word = top_p_sampling(logits, temperature, top_p)\n",
        "          generated_str += \" \" + str(word_array[last_word])\n",
        "\n",
        "    return generated_str.replace(\" . \", \". \")\n",
        "\n"
      ],
      "metadata": {
        "id": "9n5rmlp8bFfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)\n",
        "print(generate(model, seed_str='The king'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0yl8Oe2e02h",
        "outputId": "c5eeb9b1-ec01-47a7-f1e3-8ae1245d2246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The king came down. it all galloped from the windows but john saw a still well. the cat followed and devoured them with evgenie pavlovitch as happy as light and carried him away. the queen spoke several times to her son but at last she thought of her shabby\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Word embeddings\n",
        "\n",
        "An embedding layer simply projects the one-hot encoded tokens into a vector with fewer dimensions. These new 'embeddings' are like more dense versions of words or tokens. In practical terms, this embedding layer is just another linear layer with a weight matrix and it is one of the parameters the model will learn to optimize during training.\n",
        "\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/4.4%20-%20Lab%20-%20Word-Level%20RNN/embeddings.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TgW76b9gNo9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_closest_words(model, word_idx, n=10):\n",
        "    # Get the embedding for the specified word\n",
        "    word_embedding = model.embedding(torch.tensor([word_idx])).detach().numpy()\n",
        "\n",
        "    # Get all embeddings\n",
        "    all_embeddings = model.embedding.weight.detach().numpy()\n",
        "\n",
        "    # Calculate similarities (cosine similarity in this example)\n",
        "    similarities = cosine_similarity(word_embedding, all_embeddings)\n",
        "\n",
        "    # Find the indices of the most similar embeddings\n",
        "    closest_idxs = np.argsort(similarities[0])[::-1][1:n+1]  # Exclude the word itself\n",
        "\n",
        "    return closest_idxs"
      ],
      "metadata": {
        "id": "NfzITQRustRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_idx = word2int['she']  # Replace with actual word\n",
        "model.to('cpu')\n",
        "closest_words = get_closest_words(model, word_idx, 10)\n",
        "for idx in closest_words:\n",
        "    print(word_array[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJrX_5axtMzT",
        "outputId": "f6fff4c5-6441-431a-a599-809e733fc547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he\n",
            "wand\n",
            "elinor\n",
            "woman\n",
            "lavinia\n",
            "amy\n",
            "papa\n",
            "tide\n",
            "ermengarde\n",
            "tink\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing dataset"
      ],
      "metadata": {
        "id": "nZ0nzfd6KyCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nltk\n"
      ],
      "metadata": {
        "id": "bRTzVlSQFM0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import string\n",
        "\n",
        "def process_fairy_tales(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text into words and find the top 5000 words\n",
        "    words = word_tokenize(text)\n",
        "    top_5000_words = set(word for word, count in Counter(words).most_common(5000))\n",
        "\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Filter sentences\n",
        "    filtered_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Remove sentences with quoted dialogue\n",
        "        if re.search(r'[\"“”]', sentence):\n",
        "            continue\n",
        "\n",
        "        # Check if all words in the sentence are in the top 5000 words\n",
        "        sentence_words = word_tokenize(sentence)\n",
        "        if all(word in top_5000_words for word in sentence_words):\n",
        "            # Remove all punctuation except periods\n",
        "            sentence = re.sub(r'[^\\w\\s\\.]', '', sentence)\n",
        "            filtered_sentences.append(sentence)\n",
        "\n",
        "    # Join the remaining sentences\n",
        "    return ' '.join(filtered_sentences)\n",
        "\n",
        "nltk.download('punkt')\n",
        "processed_text = process_fairy_tales(text)\n",
        "len(processed_text)"
      ],
      "metadata": {
        "id": "C7Pb6WoKFQvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"fairy_tales_simple_dataset_most_common_5000_words.txt\"\n",
        "\n",
        "with open(file_name, 'w') as file:\n",
        "    file.write(processed_text)\n"
      ],
      "metadata": {
        "id": "l6n-ygxbATdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2"
      ],
      "metadata": {
        "id": "n-iMfH_wI5-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, num_layers,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(self.num_layers, batch_size, self.rnn_hidden_size)\n",
        "        return hidden.to(DEVICE), cell.to(DEVICE)\n",
        "\n",
        "vocab_size = len(word_array)\n",
        "embed_dim = 256\n",
        "\n",
        "rnn_hidden_size = 1024\n",
        "\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model = model.to(DEVICE)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9EJ9J-WI8DL",
        "outputId": "da95bb17-1ba3-4728-af25-1f19dd834f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(5371, 256)\n",
              "  (rnn): LSTM(256, 1024, num_layers=2, batch_first=True)\n",
              "  (fc): Linear(in_features=1024, out_features=5371, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)\n",
        "print(generate(model, seed_str='elinor'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbBOcwoZEZSN",
        "outputId": "c2b9fe03-fed0-4991-aefe-8b9bebd1859a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elinor order gift intimate lorrys polly humanity underneath saved tsar playing emperors bastille slid thankful parties day hags flesh could being fairies push protection remaining reign veil louder regarding news yere tent beauty engagement resolved voyage onto subjects bad stooping jarviss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JoR5YBpjJtiI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}